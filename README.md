# Numerical_Optimization_ITI
•Implemented various optimization algorithms, including Gradient 
Descent variants (Batch, Mini-Batch, SGD), Momentum, NAG, Adam, 
Adagrad, and RMSprop.
•Developed and analyzed second-order methods like Newtonʼs Method 
and BFGS, comparing convergence rates and efficiency.
