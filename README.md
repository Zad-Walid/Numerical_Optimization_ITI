# Numerical_Optimization_ITI
- **Gradient Descent Variants**: Implemented Batch, Mini-Batch, and Stochastic Gradient Descent (SGD), along with optimization techniques like Momentum, NAG, Adam, Adagrad, and RMSprop.
- **Second-Order Methods**: Developed and analyzed Newtonâ€™s Method and BFGS, comparing their convergence rates and efficiency.
